%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\label{sec:CandO}Conclusions \& Outlook}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this work, we have completed the investigation of sterile neutrino Dark Matter production from scalar decays. We have shown how to perform the full computation using Boltzmann equations on the level of momentum distribution functions. While this would in principle amount to numerically solving a coupled system of partial integro-differential equations of a rather uncommon (and possibly unknown) type, we have argued why it is allowed to decouple the equations, how to transform the partial into an ordinary integro-differential equation, and which steps to take to finally solve it using an algorithm based on a discretisation in one variable. All that comprised pioneering work, which had previously not been available without drastic assumptions or simplifications. While our findings are of course somewhat specific to the examples studied, our methods carry over to more general settings such as decays of non-scalar particles or multiple generations of sterile neutrinos: any reader inclined to investigate such scenarios can adopt our methods and will be able to apply the same strategies to crack the equations involved.

The importance of investigating sterile neutrino Dark Matter on the level of momentum distribution functions lies in these quantities carrying \emph{all} the information about the Dark Matter model at hand. Not only can they be used to extract binary information such as the Dark Matter number or energy densities, but having the distribution functions is vital to derive predictions for and apply constraints from cosmic structure formation. Many previous works tried to go around this by solving oversimplified rate equations and using a simplistic estimate of the free-streaming horizon to conclude about whether or not a setting is consistent. However, we have shown that this procedure can fail once non-thermal distribution functions are involved. In these cases, it is just not a very reliable strategy. We have instead proposed to compute the linear power spectrum (from which we derive the squared transfer function), for which task dedicated tools are available, and to use the half-mode analysis developed in this work to obtain a fairly good estimate of the compatibility of a certain distribution function with data~\cite{MenciSchneiderViel}.

Having investigated the production of sterile neutrino Dark Matter from scalar decay in full generality, the final question is where to go from here. While we have shown how to obtain a reliable constraint from Lyman-$\alpha$ data using the half-mode analysis presented in Sec.~\ref{sec:Technicalities:Bounds}, in principle, one could refine the analysis presented, as we intend to do, and/or study further halo properties of settings like the one presented, as done e.g.\ for mixed Dark Matter settings~\cite{Maccio:2012uh,Anderhalden:2012jc}. The distribution functions obtained from scalar decay are conceptually not very different, however, as we have seen, they are highly non-thermal in both shape and number of momentum scales, so that even a description by a superposition of two thermally-shaped distributions may in fact not be sufficient. Various constraints can be applied, with maybe the most generic apart from the Lyman-$\alpha$ forest being dwarf satellite galaxy counts~\cite{Schneider:2014rda}.\footnote{Note that this has nothing to do with the missing satellite problem. On the contrary, for non-cold Dark Matter, it could possibly happen that \emph{not enough} satellites are produced, instead of producing too many. This yields quite a strong bound: at least the number of observed satellites has to be met, since satellites cannot be produced from larger structures.}

The main limitation of our method is that, currently, we are using only the linear power spectrum~\cite{Ma:1995ey}, while more considerable differences of decay-produced Dark Matter to thermal spectra may be visible in the non-linear regime. Ideally, one would perform and $N$-body simulation for each distribution presented here, however, this is clearly not doable given that these computations are numerically very expensive. Thus, a better strategy is to first obtain a pre-selection of cases which could be potentially interesting for a full $N$-body simulation, while discarding those which are clearly ruled out, or in practice no different from cold Dark Matter. One way to do such an analysis is to estimate the effect of the non-linearities, by the extended Press-Schechter approach~\cite{Press:1973iz,Sheth:1999mn}, modified by incorporating a collapse model that describes how the free-streaming Dark Matter particles get bound to form structures~\cite{Schneider:2013ria,Schneider:2014rda}. This is the next step to be done, which we will leave for future work~\cite{MenciSchneiderViel}. Given that present-day data is already sufficient to yield vastly different constraints on different sterile neutrino Dark Matter production mechanisms~\cite{Merle:2014xpa} it can be expected that -- with more detailed computations -- we may be able to clearly distinguish various production mechanisms by observational data.
